{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Collaborative Filtering (NCF)\n",
    "\n",
    "This notebook serves as an introduction to Neural Collaborative Filtering (NCF), which is an innovative algorithm based on deep neural networks to tackle the key problem in recommendation — collaborative filtering — on the basis of implicit feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Global Settings and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) \n",
      "[GCC 7.3.0]\n",
      "Pandas version: 0.24.1\n",
      "Tensorflow version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import papermill as pm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from reco_utils.recommender.ncf.ncf_singlenode import NCF\n",
    "from reco_utils.recommender.ncf.dataset import Dataset as NCFDataset\n",
    "from reco_utils.dataset import movielens\n",
    "from reco_utils.dataset.python_splitters import python_chrono_split\n",
    "from reco_utils.evaluation.python_evaluation import (rmse, mae, rsquared, exp_var, map_at_k, ndcg_at_k, precision_at_k, \n",
    "                                                     recall_at_k, get_top_k_items)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top k items to recommend\n",
    "TOP_K = 10\n",
    "\n",
    "# Select Movielens data size: 100k, 1m, 10m, or 20m\n",
    "MOVIELENS_DATA_SIZE = '100k'\n",
    "\n",
    "# # Model parameters\n",
    "# EPOCHS = 200\n",
    "# BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>datetime</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>hourofday</th>\n",
       "      <th>monthofyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>881250949</td>\n",
       "      <td>1997-12-04 15:55:49</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3.0</td>\n",
       "      <td>891717742</td>\n",
       "      <td>1998-04-04 19:22:22</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1.0</td>\n",
       "      <td>878887116</td>\n",
       "      <td>1997-11-07 07:18:36</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2.0</td>\n",
       "      <td>880606923</td>\n",
       "      <td>1997-11-27 05:02:03</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>886397596</td>\n",
       "      <td>1998-02-02 05:33:16</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  rating  timestamp            datetime  dayofweek  \\\n",
       "0     196     242     3.0  881250949 1997-12-04 15:55:49          3   \n",
       "1     186     302     3.0  891717742 1998-04-04 19:22:22          5   \n",
       "2      22     377     1.0  878887116 1997-11-07 07:18:36          4   \n",
       "3     244      51     2.0  880606923 1997-11-27 05:02:03          3   \n",
       "4     166     346     1.0  886397596 1998-02-02 05:33:16          0   \n",
       "\n",
       "   hourofday  monthofyear  \n",
       "0         15           12  \n",
       "1         19            4  \n",
       "2          7           11  \n",
       "3          5           11  \n",
       "4          5            2  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=[\"userID\", \"itemID\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "df[\"datetime\"] = df[\"timestamp\"].apply(lambda x: datetime.utcfromtimestamp(x))\n",
    "df[\"dayofweek\"] = df[\"datetime\"].apply(lambda x: x.weekday())\n",
    "df[\"hourofday\"] = df[\"datetime\"].apply(lambda x: x.hour)\n",
    "df[\"monthofyear\"] = df[\"datetime\"].apply(lambda x: x.month)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = python_chrono_split(df, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dot\n",
    "from keras.models import Model\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# NN structure for pure MF\n",
    "###############\n",
    "\n",
    "user_id_input = Input(shape=[1], name='user')\n",
    "item_id_input = Input(shape=[1], name='item')\n",
    "\n",
    "embedding_size = 100\n",
    "user_count = df.userID.unique().shape[0]\n",
    "item_count = df.itemID.unique().shape[0]\n",
    "\n",
    "\n",
    "###############\n",
    "# NN structure\n",
    "###############\n",
    "\n",
    "user_embedding = Embedding(output_dim=embedding_size, input_dim=user_count,\n",
    "                           input_length=1, name='user_embedding')(user_id_input)\n",
    "item_embedding = Embedding(output_dim=embedding_size, input_dim=item_count,\n",
    "                           input_length=1, name='item_embedding')(item_id_input)\n",
    "\n",
    "user_vecs = Reshape([embedding_size])(user_embedding)\n",
    "item_vecs = Reshape([embedding_size])(item_embedding)\n",
    "\n",
    "y = Dot(1, normalize=False)([user_vecs, item_vecs])\n",
    "\n",
    "model = Model(inputs=[user_id_input, item_id_input], outputs=y)\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer=\"adam\"\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Run the network\n",
    "###############\n",
    "\n",
    "import time\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "mainpath = '/home/admin711/notebooks/tian/result/deep_beer_try_out'\n",
    "save_path = mainpath + \"/model\"\n",
    "mytime = time.strftime(\"%Y_%m_%d_%H_%M\")\n",
    "modname = 'matrix_facto_5_' + mytime \n",
    "thename = save_path + '/' + modname + '.h5'\n",
    "mcheck = ModelCheckpoint(thename, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit([train[\"userID\"], train[\"itemID\"]]\n",
    "                    , train[\"rating\"]\n",
    "                    , batch_size=64, epochs=30\n",
    "                    , validation_split=0.1\n",
    "                    , callbacks=[mcheck]\n",
    "                    , shuffle=True)\n",
    "\n",
    "import pickle\n",
    "with open(mainpath + '/histories/' + modname + '.pkl' , 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# NN structure with user and item embedding and a few more layers\n",
    "###############\n",
    "\n",
    "user_id_input = Input(shape=[1], name='user')\n",
    "item_id_input = Input(shape=[1], name='item')\n",
    "\n",
    "embedding_size = 100\n",
    "user_count = df.userID.unique().shape[0]\n",
    "item_count = df.itemID.unique().shape[0]\n",
    "\n",
    "\n",
    "###############\n",
    "# NN structure\n",
    "###############\n",
    "\n",
    "user_embedding = Embedding(output_dim=embedding_size, input_dim=user_count,\n",
    "                           input_length=1, name='user_embedding')(user_id_input)\n",
    "item_embedding = Embedding(output_dim=embedding_size, input_dim=item_count,\n",
    "                           input_length=1, name='item_embedding')(item_id_input)\n",
    "\n",
    "user_vecs = Reshape([embedding_size])(user_embedding)\n",
    "item_vecs = Reshape([embedding_size])(item_embedding)\n",
    "\n",
    "input_vecs = Concatenate()([user_vecs, item_vecs])\n",
    "\n",
    "x = Dense(32, activation='relu')(input_vecs)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "\n",
    "y = Dense(1)(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=[user_id_input, item_id_input], outputs=y)\n",
    "model.compile(optimizer='adam', loss='mse')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67492 samples, validate on 7500 samples\n",
      "Epoch 1/30\n",
      "67492/67492 [==============================] - 33s 490us/step - loss: 1.7728 - val_loss: 1.2139\n",
      "Epoch 2/30\n",
      "67492/67492 [==============================] - 4s 56us/step - loss: 0.9392 - val_loss: 1.0547\n",
      "Epoch 3/30\n",
      "67492/67492 [==============================] - 4s 58us/step - loss: 0.8768 - val_loss: 1.0304\n",
      "Epoch 4/30\n",
      "67492/67492 [==============================] - 4s 57us/step - loss: 0.8508 - val_loss: 1.0302\n",
      "Epoch 5/30\n",
      "67492/67492 [==============================] - 4s 57us/step - loss: 0.8313 - val_loss: 0.9855\n",
      "Epoch 6/30\n",
      "67492/67492 [==============================] - 4s 57us/step - loss: 0.8130 - val_loss: 1.0695\n",
      "Epoch 7/30\n",
      "67492/67492 [==============================] - 4s 57us/step - loss: 0.7959 - val_loss: 1.0828\n",
      "Epoch 8/30\n",
      "67492/67492 [==============================] - 4s 56us/step - loss: 0.7797 - val_loss: 1.0491\n",
      "Epoch 9/30\n",
      "67492/67492 [==============================] - 4s 57us/step - loss: 0.7592 - val_loss: 1.0630\n",
      "Epoch 10/30\n",
      "67492/67492 [==============================] - 4s 56us/step - loss: 0.7393 - val_loss: 1.0640\n",
      "Epoch 11/30\n",
      "67492/67492 [==============================] - 4s 57us/step - loss: 0.7238 - val_loss: 1.0692\n",
      "Epoch 12/30\n",
      "67492/67492 [==============================] - 4s 56us/step - loss: 0.7063 - val_loss: 1.0682\n",
      "Epoch 13/30\n",
      "67492/67492 [==============================] - 4s 56us/step - loss: 0.6923 - val_loss: 1.0839\n",
      "Epoch 14/30\n",
      "67492/67492 [==============================] - 4s 56us/step - loss: 0.6822 - val_loss: 1.1003\n",
      "Epoch 15/30\n",
      "67492/67492 [==============================] - 4s 56us/step - loss: 0.6721 - val_loss: 1.1188\n",
      "Epoch 16/30\n",
      "67492/67492 [==============================] - 4s 56us/step - loss: 0.6569 - val_loss: 1.1280\n",
      "Epoch 17/30\n",
      "67492/67492 [==============================] - 4s 57us/step - loss: 0.6505 - val_loss: 1.1047\n",
      "Epoch 18/30\n",
      "67492/67492 [==============================] - 4s 57us/step - loss: 0.6453 - val_loss: 1.1543\n",
      "Epoch 19/30\n",
      "67492/67492 [==============================] - 4s 56us/step - loss: 0.6292 - val_loss: 1.1315\n",
      "Epoch 20/30\n",
      "67492/67492 [==============================] - 4s 56us/step - loss: 0.6260 - val_loss: 1.1506\n",
      "Epoch 21/30\n",
      "67492/67492 [==============================] - 4s 56us/step - loss: 0.6213 - val_loss: 1.1630\n",
      "Epoch 22/30\n",
      "67492/67492 [==============================] - 4s 57us/step - loss: 0.6141 - val_loss: 1.1278\n",
      "Epoch 23/30\n",
      "67492/67492 [==============================] - 4s 57us/step - loss: 0.6087 - val_loss: 1.1394\n",
      "Epoch 24/30\n",
      "67492/67492 [==============================] - 4s 57us/step - loss: 0.6021 - val_loss: 1.1745\n",
      "Epoch 25/30\n",
      "67492/67492 [==============================] - 4s 56us/step - loss: 0.5963 - val_loss: 1.1397\n",
      "Epoch 26/30\n",
      "67492/67492 [==============================] - 4s 57us/step - loss: 0.5936 - val_loss: 1.1418\n",
      "Epoch 27/30\n",
      "67492/67492 [==============================] - 4s 58us/step - loss: 0.5865 - val_loss: 1.1681\n",
      "Epoch 28/30\n",
      "67492/67492 [==============================] - 4s 56us/step - loss: 0.5829 - val_loss: 1.1390\n",
      "Epoch 29/30\n",
      "67492/67492 [==============================] - 4s 56us/step - loss: 0.5784 - val_loss: 1.1427\n",
      "Epoch 30/30\n",
      "67492/67492 [==============================] - 4s 56us/step - loss: 0.5759 - val_loss: 1.1698\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# Run the network\n",
    "###############\n",
    "\n",
    "import time\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "mainpath = '/home/admin711/notebooks/tian/result/deep_beer_try_out'\n",
    "save_path = mainpath + \"/model\"\n",
    "mytime = time.strftime(\"%Y_%m_%d_%H_%M\")\n",
    "modname = 'matrix_facto_5_' + mytime \n",
    "thename = save_path + '/' + modname + '.h5'\n",
    "mcheck = ModelCheckpoint(thename, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit([train[\"userID\"], train[\"itemID\"]]\n",
    "                    , train[\"rating\"]\n",
    "                    , batch_size=64, epochs=30\n",
    "                    , validation_split=0.1\n",
    "                    , callbacks=[mcheck]\n",
    "                    , shuffle=True)\n",
    "\n",
    "import pickle\n",
    "with open(mainpath + '/histories/' + modname + '.pkl' , 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# NN structure with contactual information\n",
    "# Day of week (as embeding)\n",
    "###############\n",
    "\n",
    "user_id_input = Input(shape=[1], name='user')\n",
    "item_id_input = Input(shape=[1], name='item')\n",
    "dayofweek_input = Input(shape=[1], name='dayofweek')\n",
    "\n",
    "embedding_size_user = 40\n",
    "embedding_size_item = 40\n",
    "embedding_size_dayofweek = 7\n",
    "\n",
    "user_count = df.userID.unique().shape[0]\n",
    "item_count = df.itemID.unique().shape[0]\n",
    "dayofweek_count = df.dayofweek.unique().shape[0]\n",
    "\n",
    "###############\n",
    "# NN structure \n",
    "###############\n",
    "\n",
    "user_embedding = Embedding(output_dim=embedding_size_user, input_dim=user_count,\n",
    "                           input_length=1, name='user_embedding')(user_id_input)\n",
    "\n",
    "item_embedding = Embedding(output_dim=embedding_size_item, input_dim=item_count,\n",
    "                           input_length=1, name='item_embedding')(item_id_input)\n",
    "\n",
    "dayofweek_embedding = Embedding(output_dim=embedding_size_dayofweek, input_dim=dayofweek_count,\n",
    "                           input_length=1, name='dayofweek_embedding')(dayofweek_input)\n",
    "\n",
    "user_vecs = Reshape([embedding_size_user])(user_embedding)\n",
    "item_vecs = Reshape([embedding_size_item])(item_embedding)\n",
    "dayofweek_vecs = Reshape([embedding_size_dayofweek])(dayofweek_embedding)\n",
    "\n",
    "input_vecs = Concatenate()([user_vecs, item_vecs, dayofweek_vecs])\n",
    "\n",
    "x = Dropout(0.3)(input_vecs)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "\n",
    "y = Dense(1)(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=[user_id_input, item_id_input, dayofweek_input], outputs=y)\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer=\"adam\"\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67492 samples, validate on 7500 samples\n",
      "Epoch 1/30\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 2.3118 - val_loss: 1.2318\n",
      "Epoch 2/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.9964 - val_loss: 1.0549\n",
      "Epoch 3/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.9190 - val_loss: 1.0094\n",
      "Epoch 4/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.8868 - val_loss: 1.0021\n",
      "Epoch 5/30\n",
      "67492/67492 [==============================] - 2s 31us/step - loss: 0.8668 - val_loss: 0.9933\n",
      "Epoch 6/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.8560 - val_loss: 0.9908\n",
      "Epoch 7/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.8418 - val_loss: 0.9866\n",
      "Epoch 8/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.8356 - val_loss: 0.9752\n",
      "Epoch 9/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.8304 - val_loss: 0.9799\n",
      "Epoch 10/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.8220 - val_loss: 0.9835\n",
      "Epoch 11/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.8152 - val_loss: 0.9833\n",
      "Epoch 12/30\n",
      "67492/67492 [==============================] - 2s 31us/step - loss: 0.8107 - val_loss: 0.9848\n",
      "Epoch 13/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.8064 - val_loss: 0.9692\n",
      "Epoch 14/30\n",
      "67492/67492 [==============================] - 2s 31us/step - loss: 0.8000 - val_loss: 0.9753\n",
      "Epoch 15/30\n",
      "67492/67492 [==============================] - 2s 31us/step - loss: 0.7965 - val_loss: 0.9801\n",
      "Epoch 16/30\n",
      "67492/67492 [==============================] - 2s 31us/step - loss: 0.7872 - val_loss: 0.9850\n",
      "Epoch 17/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.7865 - val_loss: 1.0079\n",
      "Epoch 18/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.7782 - val_loss: 1.0023\n",
      "Epoch 19/30\n",
      "67492/67492 [==============================] - 2s 31us/step - loss: 0.7719 - val_loss: 0.9985\n",
      "Epoch 20/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.7679 - val_loss: 0.9881\n",
      "Epoch 21/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.7655 - val_loss: 1.0242\n",
      "Epoch 22/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.7601 - val_loss: 1.0185\n",
      "Epoch 23/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.7536 - val_loss: 1.0043\n",
      "Epoch 24/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.7511 - val_loss: 1.0116\n",
      "Epoch 25/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.7465 - val_loss: 1.0251\n",
      "Epoch 26/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.7415 - val_loss: 1.0082\n",
      "Epoch 27/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.7395 - val_loss: 0.9954\n",
      "Epoch 28/30\n",
      "67492/67492 [==============================] - 2s 31us/step - loss: 0.7377 - val_loss: 1.0184\n",
      "Epoch 29/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.7333 - val_loss: 0.9936\n",
      "Epoch 30/30\n",
      "67492/67492 [==============================] - 2s 30us/step - loss: 0.7339 - val_loss: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "mainpath = '/home/admin711/notebooks/tian/result/deep_beer_try_out'\n",
    "save_path = mainpath + \"/model\"\n",
    "mytime = time.strftime(\"%Y_%m_%d_%H_%M\")\n",
    "modname = 'matrix_facto_5_' + mytime \n",
    "thename = save_path + '/' + modname + '.h5'\n",
    "mcheck = ModelCheckpoint(thename, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit([train[\"userID\"], train[\"itemID\"], train[\"dayofweek\"]]\n",
    "                    , train[\"rating\"]\n",
    "                    , batch_size=128, epochs=30\n",
    "                    , validation_split=0.1\n",
    "                    , callbacks=[mcheck]\n",
    "                    , shuffle=True)\n",
    "\n",
    "import pickle\n",
    "with open(mainpath + '/histories/' + modname + '.pkl' , 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 1682 7\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# NN structure with contactual information\n",
    "# Day of week (as input to dense layer)\n",
    "# Time of day (as input to dense layer)\n",
    "###############\n",
    "\n",
    "user_id_input = Input(shape=[1], name='user')\n",
    "item_id_input = Input(shape=[1], name='item')\n",
    "dayofweek_input = Input(shape=[1], name='dayofweek')\n",
    "hourofday_input = Input(shape=[1], name='hourofday')\n",
    "monthofyear_input = Input(shape=[1], name='monthofyear')\n",
    "\n",
    "embedding_size_user = 40\n",
    "embedding_size_item = 40\n",
    "embedding_size_dayofweek = 7\n",
    "\n",
    "user_count = df.userID.unique().shape[0]\n",
    "item_count = df.itemID.unique().shape[0]\n",
    "dayofweek_count = df.dayofweek.unique().shape[0]\n",
    "print (user_count, item_count, dayofweek_count)\n",
    "\n",
    "\n",
    "\n",
    "##############\n",
    "# NN structure\n",
    "##############\n",
    "user_embedding = Embedding(output_dim=embedding_size_user, input_dim=user_count,\n",
    "                           input_length=1, name='user_embedding')(user_id_input)\n",
    "\n",
    "item_embedding = Embedding(output_dim=embedding_size_item, input_dim=item_count,\n",
    "                           input_length=1, name='item_embedding')(item_id_input)\n",
    "\n",
    "# dayofweek_embedding = Embedding(output_dim=embedding_size_dayofweek, input_dim=dayofweek_count,\n",
    "#                            input_length=1, name='dayofweek_embedding')(dayofweek_input)\n",
    "\n",
    "user_vecs = Reshape([embedding_size_user])(user_embedding)\n",
    "item_vecs = Reshape([embedding_size_item])(item_embedding)\n",
    "# dayofweek_vecs = Reshape([embedding_size_dayofweek])(dayofweek_embedding)\n",
    "\n",
    "dayofweek_dense = Dense(8, activation='relu')(dayofweek_input)\n",
    "hourofday_dense = Dense(16, activation='relu')(hourofday_input)\n",
    "monthofyear_dense = Dense(16, activation='relu')(monthofyear_input)\n",
    "\n",
    "input_vecs = Concatenate()([user_vecs, item_vecs, dayofweek_dense, hourofday_dense, monthofyear_dense])\n",
    "\n",
    "x = Dropout(0.3)(input_vecs)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "\n",
    "y = Dense(1)(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=[user_id_input, item_id_input, dayofweek_input, hourofday_input, monthofyear_input], outputs=y)\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer=\"adam\"\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 67492 samples, validate on 7500 samples\n",
      "Epoch 1/50\n",
      "67492/67492 [==============================] - 3s 47us/step - loss: 1.8787 - val_loss: 2.1188\n",
      "Epoch 2/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 1.0301 - val_loss: 1.4269\n",
      "Epoch 3/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.9327 - val_loss: 1.1258\n",
      "Epoch 4/50\n",
      "67492/67492 [==============================] - 3s 39us/step - loss: 0.8982 - val_loss: 1.1095\n",
      "Epoch 5/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.8857 - val_loss: 1.0977\n",
      "Epoch 6/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.8753 - val_loss: 1.0916\n",
      "Epoch 7/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.8695 - val_loss: 1.0285\n",
      "Epoch 8/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.8604 - val_loss: 1.1245\n",
      "Epoch 9/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.8564 - val_loss: 1.0494\n",
      "Epoch 10/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.8452 - val_loss: 1.0777\n",
      "Epoch 11/50\n",
      "67492/67492 [==============================] - 3s 40us/step - loss: 0.8373 - val_loss: 1.0310\n",
      "Epoch 12/50\n",
      "67492/67492 [==============================] - 3s 39us/step - loss: 0.8313 - val_loss: 1.0445\n",
      "Epoch 13/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.8242 - val_loss: 1.0042\n",
      "Epoch 14/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.8174 - val_loss: 1.0243\n",
      "Epoch 15/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.8127 - val_loss: 1.0111\n",
      "Epoch 16/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.8069 - val_loss: 1.0151\n",
      "Epoch 17/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.8026 - val_loss: 1.0077\n",
      "Epoch 18/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7976 - val_loss: 1.0228\n",
      "Epoch 19/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7934 - val_loss: 1.0047\n",
      "Epoch 20/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7897 - val_loss: 0.9911\n",
      "Epoch 21/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7879 - val_loss: 1.0099\n",
      "Epoch 22/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7826 - val_loss: 1.0092\n",
      "Epoch 23/50\n",
      "67492/67492 [==============================] - 3s 39us/step - loss: 0.7783 - val_loss: 1.0084\n",
      "Epoch 24/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7752 - val_loss: 0.9836\n",
      "Epoch 25/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7700 - val_loss: 0.9948\n",
      "Epoch 26/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7682 - val_loss: 0.9995\n",
      "Epoch 27/50\n",
      "67492/67492 [==============================] - 3s 39us/step - loss: 0.7644 - val_loss: 1.0041\n",
      "Epoch 28/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7620 - val_loss: 1.0029\n",
      "Epoch 29/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7573 - val_loss: 0.9929\n",
      "Epoch 30/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7552 - val_loss: 0.9990\n",
      "Epoch 31/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7504 - val_loss: 1.0028\n",
      "Epoch 32/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7438 - val_loss: 1.0244\n",
      "Epoch 33/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7412 - val_loss: 1.0076\n",
      "Epoch 34/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7382 - val_loss: 1.0235\n",
      "Epoch 35/50\n",
      "67492/67492 [==============================] - 3s 37us/step - loss: 0.7366 - val_loss: 1.0149\n",
      "Epoch 36/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7334 - val_loss: 1.0268\n",
      "Epoch 37/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7300 - val_loss: 1.0151\n",
      "Epoch 38/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7274 - val_loss: 1.0079\n",
      "Epoch 39/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7252 - val_loss: 1.0302\n",
      "Epoch 40/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7242 - val_loss: 1.0061\n",
      "Epoch 41/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7224 - val_loss: 1.0358\n",
      "Epoch 42/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7200 - val_loss: 1.0124\n",
      "Epoch 43/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7178 - val_loss: 1.0115\n",
      "Epoch 44/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7165 - val_loss: 1.0602\n",
      "Epoch 45/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7141 - val_loss: 1.0404\n",
      "Epoch 46/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7125 - val_loss: 1.0147\n",
      "Epoch 47/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7133 - val_loss: 1.0317\n",
      "Epoch 48/50\n",
      "67492/67492 [==============================] - 3s 37us/step - loss: 0.7092 - val_loss: 1.0265\n",
      "Epoch 49/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7129 - val_loss: 1.0332\n",
      "Epoch 50/50\n",
      "67492/67492 [==============================] - 3s 38us/step - loss: 0.7073 - val_loss: 1.0293\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "mainpath = '/home/admin711/notebooks/tian/result/deep_beer_try_out'\n",
    "save_path = mainpath + \"/model\"\n",
    "mytime = time.strftime(\"%Y_%m_%d_%H_%M\")\n",
    "modname = 'matrix_facto_5_' + mytime \n",
    "thename = save_path + '/' + modname + '.h5'\n",
    "mcheck = ModelCheckpoint(thename, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit([train[\"userID\"], train[\"itemID\"], train[\"dayofweek\"], train[\"hourofday\"], train[\"monthofyear\"]]\n",
    "                    , train[\"rating\"]\n",
    "                    , batch_size=128, epochs=50\n",
    "                    , validation_split=0.1\n",
    "                    , callbacks=[mcheck]\n",
    "                    , shuffle=True)\n",
    "\n",
    "import pickle\n",
    "with open(mainpath + '/histories/' + modname + '.pkl' , 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "# from keras.utils import plot_model\n",
    "# import pydot\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss'])\n",
      "dict_keys(['val_loss', 'loss'])\n"
     ]
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "1\n",
    "2\n",
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Visualize training history\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "# # summarize history for accuracy\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25008/25008 [==============================] - 0s 11us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9968751991550204"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############\n",
    "# Evaluate using keras evaluate()\n",
    "##############\n",
    "\n",
    "x_test = [test[\"userID\"], test[\"itemID\"], test[\"dayofweek\"], test[\"hourofday\"], test[\"monthofyear\"]]\n",
    "evaluate_loss = model.evaluate(x=x_test, y=test[\"rating\"], batch_size=128)\n",
    "evaluate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9968751952079705"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############\n",
    "# Evaluate using keras predict() and keras metrics\n",
    "##############\n",
    "\n",
    "from keras import metrics\n",
    "import keras\n",
    "\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "mse = metrics.mean_squared_error(test[\"rating\"], y_test_pred.flatten())\n",
    "tf_session = keras.backend.get_session()\n",
    "mse.eval(session=tf_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9968751952079861"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############\n",
    "# Evaluate using keras predict() and sklearn metrics\n",
    "##############\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test[\"rating\"], y_test_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Matrix factorization algorithm\n",
    "\n",
    "NCF is new neural matrix factorization model, which ensembles Generalized Matrix Factorization (GMF) and Multi-Layer Perceptron (MLP) to unify the strengths of linearity of MF and non-linearity of MLP for modelling the user–item latent structures. NCF can be demonstrated as a framework for GMF and MLP, which is illustrated as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://recodatasets.blob.core.windows.net/images/NCF.svg?sanitize=true\">\n",
    "\n",
    "This figure shows how to utilize latent vectors of items and users, and then how to fuse outputs from GMF Layer (left) and MLP Layer (right). We will introduce this framework and show how to learn the model parameters in following sections.\n",
    "\n",
    "### 1.1 The GMF model\n",
    "\n",
    "In ALS, the ratings are modeled as follows:\n",
    "\n",
    "$$\\hat { r } _ { u , i } = q _ { i } ^ { T } p _ { u }$$\n",
    "\n",
    "GMF introduces neural CF layer as the output layer of standard MF. In this way, MF can be easily generalized\n",
    "and extended. For example, if we allow the edge weights of this output layer to be learnt from data without the uniform constraint, it will result in a variant of MF that allows varying importance of latent dimensions. And if we use a non-linear function for activation, it will generalize MF to a non-linear setting which might be more expressive than the linear MF model. GMF can be shown as follows:\n",
    "\n",
    "$$\\hat { r } _ { u , i } = a _ { o u t } \\left( h ^ { T } \\left( q _ { i } \\odot p _ { u } \\right) \\right)$$\n",
    "\n",
    "where $\\odot$ is element-wise product of vectors. Additionally, ${a}_{out}$ and ${h}$ denote the activation function and edge weights of the output layer respectively. MF can be interpreted as a special case of GMF. Intuitively, if we use an identity function for aout and enforce h to be a uniform vector of 1, we can exactly recover the MF model.\n",
    "\n",
    "### 1.2 The MLP model\n",
    "\n",
    "NCF adopts two pathways to model users and items: 1) element-wise product of vectors, 2) concatenation of vectors. To learn interactions after concatenating of users and items latent features, the standard MLP model is applied. In this sense, we can endow the model a large level of flexibility and non-linearity to learn the interactions between $p_{u}$ and $q_{i}$. The details of MLP model are:\n",
    "\n",
    "For the input layer, there is concatention of user and item vectors:\n",
    "\n",
    "$$z _ { 1 } = \\phi _ { 1 } \\left( p _ { u } , q _ { i } \\right) = \\left[ \\begin{array} { c } { p _ { u } } \\\\ { q _ { i } } \\end{array} \\right]$$\n",
    "\n",
    "So for the hidden layers and output layer of MLP, the details are:\n",
    "\n",
    "$$\n",
    "\\phi _ { l } \\left( z _ { l } \\right) = a _ { o u t } \\left( W _ { l } ^ { T } z _ { l } + b _ { l } \\right) , ( l = 2,3 , \\ldots , L - 1 )\n",
    "$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$\n",
    "\\hat { r } _ { u , i } = \\sigma \\left( h ^ { T } \\phi \\left( z _ { L - 1 } \\right) \\right)\n",
    "$$\n",
    "\n",
    "where ${ W }_{ l }$, ${ b }_{ l }$, and ${ a }_{ out }$ denote the weight matrix, bias vector, and activation function for the $l$-th layer’s perceptron, respectively. For activation functions of MLP layers, one can freely choose sigmoid, hyperbolic tangent (tanh), and Rectifier (ReLU), among others. Because of implicit data task, the activation function of the output layer is defined as sigmoid $\\sigma(x)=\\frac{1}{1+e^{-x}}$ to restrict the predicted score to be in (0,1).\n",
    "\n",
    "\n",
    "### 1.3 Fusion of GMF and MLP\n",
    "\n",
    "To provide more flexibility to the fused model, we allow GMF and MLP to learn separate embeddings, and combine the two models by concatenating their last hidden layer. We get $\\phi^{GMF}$ from GMF:\n",
    "\n",
    "$$\\phi _ { u , i } ^ { G M F } = p _ { u } ^ { G M F } \\odot q _ { i } ^ { G M F }$$\n",
    "\n",
    "and obtain $\\phi^{MLP}$ from MLP:\n",
    "\n",
    "$$\\phi _ { u , i } ^ { M L P } = a _ { o u t } \\left( W _ { L } ^ { T } \\left( a _ { o u t } \\left( \\ldots a _ { o u t } \\left( W _ { 2 } ^ { T } \\left[ \\begin{array} { c } { p _ { u } ^ { M L P } } \\\\ { q _ { i } ^ { M L P } } \\end{array} \\right] + b _ { 2 } \\right) \\ldots \\right) \\right) + b _ { L }\\right.$$\n",
    "\n",
    "Lastly, we fuse output from GMF and MLP:\n",
    "\n",
    "$$\\hat { r } _ { u , i } = \\sigma \\left( h ^ { T } \\left[ \\begin{array} { l } { \\phi ^ { G M F } } \\\\ { \\phi ^ { M L P } } \\end{array} \\right] \\right)$$\n",
    "\n",
    "This model combines the linearity of MF and non-linearity of DNNs for modelling user–item latent structures.\n",
    "\n",
    "### 1.4 Objective Function\n",
    "\n",
    "We define the likelihood function as:\n",
    "\n",
    "$$P \\left( \\mathcal { R } , \\mathcal { R } ^ { - } | \\mathbf { P } , \\mathbf { Q } , \\Theta \\right) = \\prod _ { ( u , i ) \\in \\mathcal { R } } \\hat { r } _ { u , i } \\prod _ { ( u , j ) \\in \\mathcal { R } ^{ - } } \\left( 1 - \\hat { r } _ { u , j } \\right)$$\n",
    "\n",
    "Where $\\mathcal{R}$ denotes the set of observed interactions, and $\\mathcal{ R } ^ { - }$ denotes the set of negative instances. $\\mathbf{P}$ and $\\mathbf{Q}$ denotes the latent factor matrix for users and items, respectively; and $\\Theta$ denotes the model parameters. Taking the negative logarithm of the likelihood, we obatain the objective function to minimize for NCF method, which is known as [binary cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy):\n",
    "\n",
    "$$L = - \\sum _ { ( u , i ) \\in \\mathcal { R } \\cup { \\mathcal { R } } ^ { - } } r _ { u , i } \\log \\hat { r } _ { u , i } + \\left( 1 - r _ { u , i } \\right) \\log \\left( 1 - \\hat { r } _ { u , i } \\right)$$\n",
    "\n",
    "The optimization can be done by performing Stochastic Gradient Descent (SGD), which is described in the [Surprise SVD deep dive notebook](../02_model/surprise_svd_deep_dive.ipynb). Our SGD method is very similar to the SVD algorithm's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 TensorFlow implementation of NCF\n",
    "\n",
    "We will use the Movielens dataset, which is composed of integer ratings from 1 to 5.\n",
    "\n",
    "We convert Movielens into implicit feedback, and evaluate under our *leave-one-out* evaluation protocol.\n",
    "\n",
    "You can check the details of implementation in `reco_utils/recommender/ncf`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 TensorFlow NCF movie recommender\n",
    "\n",
    "### 3.1 Load and split data\n",
    "\n",
    "To evaluate the performance of item recommendation, we adopted the leave-one-out evaluation.\n",
    "\n",
    "For each user, we held out his/her latest interaction as the test set and utilized the remaining data for training. We use `python_chrono_split` to achieve this. And since it is too time-consuming to rank all items for every user during evaluation, we followed the common strategy that randomly samples 100 items that are not interacted by the user, ranking the test item among the 100 items. Our test samples will be constructed by `NCFDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3.0</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1.0</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2.0</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  rating  timestamp\n",
       "0     196     242     3.0  881250949\n",
       "1     186     302     3.0  891717742\n",
       "2      22     377     1.0  878887116\n",
       "3     244      51     2.0  880606923\n",
       "4     166     346     1.0  886397596"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = movielens.load_pandas_df(\n",
    "    size=MOVIELENS_DATA_SIZE,\n",
    "    header=[\"userID\", \"itemID\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = python_chrono_split(df, 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Functions of NCF Dataset \n",
    "\n",
    "Dataset Class for NCF, where important functions are:\n",
    "\n",
    "`negative_sampling()`, sample negative user & item pair for every positive instances, with parameter `n_neg`.\n",
    "\n",
    "`train_loader(batch_size, shuffle=True)`, generate training batch with `batch_size`, also we can set whether `shuffle` this training set.\n",
    "\n",
    "`test_loader()`, generate test batch by every positive test instance, (eg. \\[1, 2, 1\\] is a positive user & item pair in test set (\\[userID, itemID, rating\\] for this tuple). This function returns like \\[\\[1, 2, 1\\], \\[1, 3, 0\\], \\[1,6, 0\\], ...\\], ie. following our *leave-one-out* evaluation protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = NCFDataset(train=train, test=test, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train NCF based on TensorFlow\n",
    "The NCF has a lot of parameters. The most important ones are:\n",
    "\n",
    "`n_factors`, which controls the dimension of the latent space. Usually, the quality of the training set predictions grows with as n_factors gets higher.\n",
    "\n",
    "`layer_sizes`, sizes of input layer (and hidden layers) of MLP, input type is list.\n",
    "\n",
    "`n_epochs`, which defines the number of iteration of the SGD procedure.\n",
    "Note that both parameter also affect the training time.\n",
    "\n",
    "`model_type`, we can train single `\"MLP\"`, `\"GMF\"` or combined model `\"NCF\"` by changing the type of model.\n",
    "\n",
    "We will here set `n_factors` to `4`, `layer_sizes` to `[16,8,4]`,  `n_epochs` to `100`, `batch_size` to 256. To train the model, we simply need to call the `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NCF (\n",
    "    n_users=data.n_users, \n",
    "    n_items=data.n_items,\n",
    "    model_type=\"NeuMF\",\n",
    "    n_factors=4,\n",
    "    layer_sizes=[16,8,4],\n",
    "    n_epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=1e-3,\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: neumf\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(data)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Prediction\n",
    "\n",
    "Now that our model is fitted, we can call `predict` to get some `predictions`. `predict` returns an internal object Prediction which can be easily converted back to a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.684982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.032923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.026047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>0.864664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.327991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  prediction\n",
       "0     1.0    88.0    0.684982\n",
       "1     1.0   149.0    0.032923\n",
       "2     1.0   103.0    0.026047\n",
       "3     1.0   239.0    0.864664\n",
       "4     1.0   110.0    0.327991"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [[row.userID, row.itemID, model.predict(row.userID, row.itemID)]\n",
    "               for (_, row) in test.iterrows()]\n",
    "\n",
    "test_time = time.time() - start_time\n",
    "\n",
    "predictions = pd.DataFrame(predictions, columns=['userID', 'itemID', 'prediction'])\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Generic Evaluation\n",
    "We remove rated movies in the top k recommendations\n",
    "To compute ranking metrics, we need predictions on all user, item pairs. We remove though the items already watched by the user, since we choose not to recommend them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3.354566812515259 seconds for prediction.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "users, items, preds = [], [], []\n",
    "item = list(train.itemID.unique())\n",
    "for user in train.userID.unique():\n",
    "    user = [user] * len(item) \n",
    "    users.extend(user)\n",
    "    items.extend(item)\n",
    "    preds.extend(list(model.predict(user, item, is_list=True)))\n",
    "\n",
    "all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n",
    "\n",
    "merged = pd.merge(train, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n",
    "all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n",
    "\n",
    "test_time = time.time() - start_time\n",
    "print(\"Took {} seconds for prediction.\".format(test_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:\t0.046610\n",
      "NDCG:\t0.191637\n",
      "Precision@K:\t0.171156\n",
      "Recall@K:\t0.096182\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eval_map = map_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "eval_ndcg = ndcg_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "eval_precision = precision_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "eval_recall = recall_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "\n",
    "print(\"MAP:\\t%f\" % eval_map,\n",
    "      \"NDCG:\\t%f\" % eval_ndcg,\n",
    "      \"Precision@K:\\t%f\" % eval_precision,\n",
    "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 \"Leave-one-out\" Evaluation\n",
    "\n",
    "We implement the functions to repoduce the leave-one-out evaluation protocol mentioned in original NCF paper.\n",
    "\n",
    "For each item in test data, we randomly samples 100 items that are not interacted by the user, ranking the test item among the 101 items (1 positive item and 100 negative items). The performance of a ranked list is judged by **Hit Ratio (HR)** and **Normalized Discounted Cumulative Gain (NDCG)**. Finally, we average the values of those ranked lists to obtain the overall HR and NDCG on test data.\n",
    "\n",
    "We truncated the ranked list at 10 for both metrics. As such, the HR intuitively measures whether the test item is present on the top-10 list, and the NDCG accounts for the position of the hit by assigning higher scores to hits at top ranks.\n",
    "\n",
    "**Note 1:** In exact leave-one-out evaluation protocol, we select only one of the latest items interacted with a user as test data for each user. But in this notebook, to compare with other algorithms, we select latest 25% dataset as test data. So this is an artificial \"leave-one-out\" evaluation only showing how to use `test_loader` and how to calculate metrics like the original paper. You can reproduce the real leave-one-out evaluation by changing the way of splitting data.\n",
    "\n",
    "**Note 2:** Because of sampling 100 negative items for each positive test item, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR:\t0.476767\n",
      "NDCG:\t0.375043\n"
     ]
    }
   ],
   "source": [
    "k = TOP_K\n",
    "\n",
    "ndcgs = []\n",
    "hit_ratio = []\n",
    "\n",
    "for b in data.test_loader():\n",
    "    user_input, item_input, labels = b\n",
    "    output = model.predict(user_input, item_input, is_list=True)\n",
    "\n",
    "    output = np.squeeze(output)\n",
    "    rank = sum(output >= output[0])\n",
    "    if rank <= k:\n",
    "        ndcgs.append(1 / np.log(rank + 1))\n",
    "        hit_ratio.append(1)\n",
    "    else:\n",
    "        ndcgs.append(0)\n",
    "        hit_ratio.append(0)\n",
    "\n",
    "eval_ndcg = np.mean(ndcgs)\n",
    "eval_hr = np.mean(hit_ratio)\n",
    "\n",
    "print(\"HR:\\t%f\" % eval_hr)\n",
    "print(\"NDCG:\\t%f\" % eval_ndcg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Pre-training\n",
    "\n",
    "To get better performance of NeuMF, we can adopt pre-training strategy. We first train GMF and MLP with random initializations until convergence. Then use their model parameters as the initialization for the corresponding parts of NeuMF’s parameters.  Please pay attention to the output layer, where we concatenate weights of the two models with\n",
    "\n",
    "$$h ^ { N C F } \\leftarrow \\left[ \\begin{array} { c } { \\alpha h ^ { G M F } } \\\\ { ( 1 - \\alpha ) h ^ { M L P } } \\end{array} \\right]$$\n",
    "\n",
    "where $h^{GMF}$ and $h^{MLP}$ denote the $h$ vector of the pretrained GMF and MLP model, respectively; and $\\alpha$ is a\n",
    "hyper-parameter determining the trade-off between the two pre-trained models. We set $\\alpha$ = 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Training GMF and MLP model\n",
    "`model.save`, we can set the `dir_name` to store the parameters of GMF and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NCF (\n",
    "    n_users=data.n_users, \n",
    "    n_items=data.n_items,\n",
    "    model_type=\"GMF\",\n",
    "    n_factors=4,\n",
    "    layer_sizes=[16,8,4],\n",
    "    n_epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=1e-3,\n",
    "    verbose=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: gmf\n",
      "Epoch 10 [6.15s]: train_loss = 0.315140 \n",
      "Epoch 20 [5.32s]: train_loss = 0.291002 \n",
      "Epoch 30 [5.03s]: train_loss = 0.272337 \n",
      "Epoch 40 [5.25s]: train_loss = 0.268146 \n",
      "Epoch 50 [6.52s]: train_loss = 0.267027 \n",
      "Epoch 60 [5.32s]: train_loss = 0.267011 \n",
      "Epoch 70 [6.75s]: train_loss = 0.264637 \n",
      "Epoch 80 [6.00s]: train_loss = 0.265867 \n",
      "Epoch 90 [5.73s]: train_loss = 0.264636 \n",
      "Epoch 100 [4.88s]: train_loss = 0.265541 \n",
      "Epoch 110 [5.21s]: train_loss = 0.265065 \n",
      "Epoch 120 [4.91s]: train_loss = 0.264575 \n",
      "Epoch 130 [4.83s]: train_loss = 0.263862 \n",
      "Epoch 140 [5.16s]: train_loss = 0.264803 \n",
      "Epoch 150 [4.94s]: train_loss = 0.264735 \n",
      "Epoch 160 [5.24s]: train_loss = 0.264377 \n",
      "Epoch 170 [5.19s]: train_loss = 0.264071 \n",
      "Epoch 180 [4.92s]: train_loss = 0.263614 \n",
      "Epoch 190 [4.93s]: train_loss = 0.263399 \n",
      "Epoch 200 [4.88s]: train_loss = 0.263900 \n",
      "Took 1042.1958258152008 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(data)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time))\n",
    "\n",
    "model.save(dir_name=\".pretrain/GMF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NCF (\n",
    "    n_users=data.n_users, \n",
    "    n_items=data.n_items,\n",
    "    model_type=\"MLP\",\n",
    "    n_factors=4,\n",
    "    layer_sizes=[16,8,4],\n",
    "    n_epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=1e-3,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: mlp\n",
      "Epoch 10 [6.32s]: train_loss = 0.329718 \n",
      "Epoch 20 [6.08s]: train_loss = 0.296939 \n",
      "Epoch 30 [6.40s]: train_loss = 0.288913 \n",
      "Epoch 40 [6.47s]: train_loss = 0.281946 \n",
      "Epoch 50 [6.08s]: train_loss = 0.278271 \n",
      "Epoch 60 [6.09s]: train_loss = 0.276184 \n",
      "Epoch 70 [5.85s]: train_loss = 0.274575 \n",
      "Epoch 80 [6.17s]: train_loss = 0.272878 \n",
      "Epoch 90 [6.03s]: train_loss = 0.271775 \n",
      "Epoch 100 [5.79s]: train_loss = 0.271446 \n",
      "Epoch 110 [5.87s]: train_loss = 0.270495 \n",
      "Epoch 120 [5.71s]: train_loss = 0.270673 \n",
      "Epoch 130 [5.79s]: train_loss = 0.269753 \n",
      "Epoch 140 [5.70s]: train_loss = 0.269657 \n",
      "Epoch 150 [5.66s]: train_loss = 0.269295 \n",
      "Epoch 160 [5.77s]: train_loss = 0.268800 \n",
      "Epoch 170 [5.63s]: train_loss = 0.267856 \n",
      "Epoch 180 [5.72s]: train_loss = 0.267994 \n",
      "Epoch 190 [5.86s]: train_loss = 0.267806 \n",
      "Epoch 200 [5.57s]: train_loss = 0.267920 \n",
      "Took 1199.5014789104462 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "model.fit(data)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time))\n",
    "\n",
    "model.save(dir_name=\".pretrain/MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 Load pre-trained GMF and MLP model for NeuMF\n",
    "`model.load`, we can set the `gmf_dir` and `mlp_dir` to store the parameters for NeuMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NCF (\n",
    "    n_users=data.n_users, \n",
    "    n_items=data.n_items,\n",
    "    model_type=\"NeuMF\",\n",
    "    n_factors=4,\n",
    "    layer_sizes=[16,8,4],\n",
    "    n_epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=1e-3,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "model.load(gmf_dir=\".pretrain/GMF\", mlp_dir=\".pretrain/MLP\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from .pretrain/GMF/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from .pretrain/MLP/model.ckpt\n",
      "Training model: neumf\n",
      "Epoch 10 [6.52s]: train_loss = 0.229828 \n",
      "Epoch 20 [6.62s]: train_loss = 0.223956 \n",
      "Epoch 30 [7.37s]: train_loss = 0.221767 \n",
      "Epoch 40 [6.70s]: train_loss = 0.219329 \n",
      "Epoch 50 [6.68s]: train_loss = 0.218969 \n",
      "Epoch 60 [6.64s]: train_loss = 0.217863 \n",
      "Epoch 70 [6.63s]: train_loss = 0.217433 \n",
      "Epoch 80 [6.41s]: train_loss = 0.215627 \n",
      "Epoch 90 [6.46s]: train_loss = 0.215249 \n",
      "Epoch 100 [6.47s]: train_loss = 0.215688 \n",
      "Epoch 110 [6.49s]: train_loss = 0.215189 \n",
      "Epoch 120 [7.85s]: train_loss = 0.213904 \n",
      "Epoch 130 [6.69s]: train_loss = 0.213772 \n",
      "Epoch 140 [8.85s]: train_loss = 0.214469 \n",
      "Epoch 150 [6.75s]: train_loss = 0.214124 \n",
      "Epoch 160 [6.84s]: train_loss = 0.213270 \n",
      "Epoch 170 [7.15s]: train_loss = 0.213109 \n",
      "Epoch 180 [7.07s]: train_loss = 0.213228 \n",
      "Epoch 190 [7.09s]: train_loss = 0.212264 \n",
      "Epoch 200 [6.88s]: train_loss = 0.212478 \n",
      "Took 1373.5731191635132 seconds for training.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(data)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(\"Took {} seconds for training.\".format(train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 Compare with not pre-trained NeuMF\n",
    "\n",
    "You can use beforementioned evaluation methods to evaluate the pre-trained `NCF` Model. Usually, we will find the performance of pre-trained NCF is better than the not pre-trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 3.304403066635132 seconds for prediction.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "users, items, preds = [], [], []\n",
    "item = list(train.itemID.unique())\n",
    "for user in train.userID.unique():\n",
    "    user = [user] * len(item) \n",
    "    users.extend(user)\n",
    "    items.extend(item)\n",
    "    preds.extend(list(model.predict(user, item, is_list=True)))\n",
    "\n",
    "all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n",
    "\n",
    "merged = pd.merge(train, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n",
    "all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n",
    "\n",
    "test_time = time.time() - start_time\n",
    "print(\"Took {} seconds for prediction.\".format(test_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP:\t0.049452\n",
      "NDCG:\t0.202544\n",
      "Precision@K:\t0.180594\n",
      "Recall@K:\t0.100380\n"
     ]
    }
   ],
   "source": [
    "eval_map2 = map_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "eval_ndcg2 = ndcg_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "eval_precision2 = precision_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "eval_recall2 = recall_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "\n",
    "print(\"MAP:\\t%f\" % eval_map2,\n",
    "      \"NDCG:\\t%f\" % eval_ndcg2,\n",
    "      \"Precision@K:\\t%f\" % eval_precision2,\n",
    "      \"Recall@K:\\t%f\" % eval_recall2, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record results with papermill for tests\n",
    "pm.record(\"map\", eval_map)\n",
    "pm.record(\"ndcg\", eval_ndcg)\n",
    "pm.record(\"precision\", eval_precision)\n",
    "pm.record(\"recall\", eval_recall)\n",
    "pm.record(\"map2\", eval_map2)\n",
    "pm.record(\"ndcg2\", eval_ndcg2)\n",
    "pm.record(\"precision2\", eval_precision2)\n",
    "pm.record(\"recall2\", eval_recall2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.4 Delete pre-trained directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did '.pretrain' exist?: False\n"
     ]
    }
   ],
   "source": [
    "save_dir = \".pretrain\"\n",
    "if os.path.exists(save_dir):\n",
    "    shutil.rmtree(save_dir)\n",
    "    \n",
    "print(\"Did \\'%s\\' exist?: %s\" % (save_dir, os.path.exists(save_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: \n",
    "1. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu & Tat-Seng Chua Neural Collaborative Filtering: https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf\n",
    "\n",
    "2. Official NCF implementation [Keras with Theano]: https://github.com/hexiangnan/neural_collaborative_filtering\n",
    "\n",
    "3. Other nice NCF implementation [Pytorch]: https://github.com/LaceyChen17/neural-collaborative-filtering"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python (reco_gpu)",
   "language": "python",
   "name": "reco_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
